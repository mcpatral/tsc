trigger: none

parameters:
  - name: environmentName
    displayName: Variable group name for environment to deploy
    type: string
    default: "dev"
  - name: systemSchemaNames
    displayName: "Systems list: comma separated e.g. system_9999,system_9998,system_9997"
    type: string
  - name: initCommonTables
    type: boolean
    default: false
    displayName: "Init common tables: false equals 0 / true equals 1"
  - name: forceDelete
    type: boolean
    default: false         

variables:
  - group: ${{ parameters.environmentName }}
  - name: terraformVersion
    value: "1.5.0"
  - name: azureServiceConnectionName
    value: sc_datamngtdev_poc
  - name: azureDevOpsPool
    value: ${{ parameters.environmentName }}-agentpool
  - name: locationInfraShort
    value: $(terraform_in_location_short)
  - name: infrastructureModule
    value: infrastructure
  - name: contentModule
    value: content
  - name: terraformResourceGroupName
    value: rg-$(terraform_in_environment_type)-$(terraform_in_project)-$(terraform_in_location_short)-state
  - name: terraformSaName
    value: sa$(terraform_in_environment_type)$(terraform_in_project)$(terraform_in_location_short)tfstate
  - name: terraformBackendContainerName
    value: $(terraform_in_environment_type)tfstate
  - name: mainResourceGroupName
    value: rg-$(terraform_in_environment_type)-$(terraform_in_project)-$(locationInfraShort)
  # Databricks upload parameters notebooks
  - name: notebookName
    value: system_initialization
  - name: databricksRemotePath
    value: '/Shared/'
  - name: wheelsPackagesRemoteFileStore
    value: '/dbfs/FileStore/wheels/'
  - name: devopsRepositoryCheckout
    value: 'self'
  - name: sharedResourcesRepositoryCheckout
    value: 'shared-resources'
  # Databricks upload parameters libraries
  - name: csvFilePath
    value: 'shared-resources/libs_version/databricks_packages.csv'

resources:
  repositories:
  - repository: shared-resources
    type: git
    name: igtpoc/shared-resources
    ref: main

stages:
  - template: azure_pipelines/databricks-run-notebooks-stages.yml
    parameters:
      azureServiceConnectionName: ${{ variables.azureServiceConnectionName }}
      terraformVersion: ${{ variables.terraformVersion }}
      terraformResourceGroupName: ${{ variables.terraformResourceGroupName }}
      terraformSaName: ${{ variables.terraformSaName }}
      terraformBackendContainerName: ${{ variables.terraformBackendContainerName }}
      mainResourceGroupName: ${{ variables.mainResourceGroupName }}
      infrastructureModule: ${{ variables.infrastructureModule }}
      contentModule: ${{ variables.contentModule }}
      notebookName: ${{ variables.notebookName }}
      environmentName: $(terraform_in_environment_type)
      environmentNameUnityCatalogGroups: $(terraform_in_unity_catalog_environment_type)
      systemSchemaNames: ${{ parameters.systemSchemaNames }}
      storageAccountName: sa$(terraform_in_environment_type)$(terraform_in_project)$(terraform_in_location_short)dl
      databricksRemotePath: ${{ variables.databricksRemotePath }}
      databricksViewerGroups: $(databricks_system_job_viewer_groups)
      wheelsPackagesRemoteFileStore: ${{ variables.wheelsPackagesRemoteFileStore }}
      wheelsCsvFile: ${{ variables.csvFilePath }}
      devopsRepositoryCheckout: ${{ variables.devopsRepositoryCheckout }}
      sharedResourcesRepositoryCheckout: ${{ variables.sharedResourcesRepositoryCheckout }}
      workingDirectory: $(System.DefaultWorkingDirectory)
      pool: ${{ variables.azureDevOpsPool }}
      initCommonTables: ${{ parameters.initCommonTables }}
      forceDelete: ${{ parameters.forceDelete }}
      condition: and(not(failed()), not(canceled()))
